{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1199a947-e269-46ba-9176-b3f1b919a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_2128/553333188.py:276: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  wrangle(file, DatesToWeek_DF, 1)\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_2128/553333188.py:276: DtypeWarning: Columns (12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  wrangle(file, DatesToWeek_DF, 1)\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_2128/553333188.py:276: DtypeWarning: Columns (5,6,7,8,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  wrangle(file, DatesToWeek_DF, 1)\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_2128/553333188.py:282: DtypeWarning: Columns (5,6,7,8,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  wrangle(file, DatesToWeek_DF, 3)\n",
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_2128/553333188.py:285: DtypeWarning: Columns (5,6,7,8,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  wrangle(file, DatesToWeek_DF, 4)\n"
     ]
    }
   ],
   "source": [
    "### CLIENT USAGE PREDICTOR (LR) - initial training script\n",
    "### Code produced by Lang (Ron) Chen Nov-Feb 2021 for Lucidity Software\n",
    "\"\"\" Wrangles initial raw data and outputs predictor objects for predicting the client usage trend for the upcoming week \"\"\"\n",
    "\n",
    "# Input: \n",
    "#    Argument 1: a cut-off date for data; logically should be a Sunday. Must be in form ‘[d]d/[m]m/yyyy’ (If day or month single digit can input just as single digit). \n",
    "#         please ensure date is valid and is after the FIRSTDATE (by default July 3rd 2017)\n",
    "\n",
    "#     Initial raw data files need to be stored in directory ‘./Data’. \n",
    "#     -File names must be ‘action.csv’, ‘assets.csv’, ‘attachment.csv’, ‘competency_record.csv’, ‘form_record.csv’, ‘form_template.csv’, ‘incident.csv’, ‘users.csv’, associated with the data of the corresponding filename\n",
    "#     -Each csv must include columns that include ‘domain’ for company name, and ‘created_at’. \n",
    "#     -There should be no other tester domains in the data apart from ‘demo’, ‘demo_2’ and ‘cruse’\n",
    "\n",
    "#     -the dates for 'action.csv', 'competency_record.csv', 'form_record.csv', 'incident.csv', 'users.csv' should be in form of [d]d/[m]m/yyyy\n",
    "#     -the dates for 'assets.csv', 'form_template.csv' should be in form of yyyy-mm-dd.\n",
    "#     *if the form of these are different then need to edit PART 2 in the script. \n",
    "\n",
    "\n",
    "# Output: several partial outputs - partial outputs of wrangled data, Linear Regression objects (.pickle), Statistics.csv, exported to various directories including the home directory (relatively '..'), the History directory (into the relevent week) and the current directory \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PART 0: IMPORTING LIBRARIES\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math as m\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PART 1: CREATE DATE FILE\n",
    "\n",
    "# Reads in 'cut-off date' for data to be used to train \n",
    "currdate = '15/8/2021'\n",
    "\n",
    "# Assumes that date format in 'dd/mm/yyyy' format\n",
    "cutoffday = int(currdate.split('/')[0])\n",
    "cutoffmonth = int(currdate.split('/')[1])\n",
    "cutoffyear = int(currdate.split('/')[2])\n",
    "\n",
    "def datejoin(day, month, year):\n",
    "    \"\"\" For joining up three numbers into a date\"\"\"\n",
    "    return (f'{str(day)}/{str(month)}/{str(year)}')\n",
    "\n",
    "\n",
    "def leapyear(year):\n",
    "    \"\"\" For determining whether a year is a leap year\"\"\"\n",
    "    if year % 4 == 0:\n",
    "        if year% 100 == 0:\n",
    "            if year%400 == 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Creates a dictionary matching each day to a week number (counting Week of July 3rd 2021 as Week 1)    \n",
    "#### FUTURE CHANGE: if wish to include data earlier than Monday July 3rd 2017, change the magic string FIRSTDATE\n",
    "FIRSTDATE = '03/07/2017'\n",
    "firstdateday = int(FIRSTDATE.split('/')[0])\n",
    "firstdatemonth = int(FIRSTDATE.split('/')[1])\n",
    "firstdateyear = int(FIRSTDATE.split('/')[2])\n",
    "\n",
    "days = [29, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]    \n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "years = range(2017, cutoffyear+1)\n",
    "\n",
    "\n",
    "datematchweek = dict()\n",
    "week = 1\n",
    "count = 0\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        \n",
    "        if (year == firstdateyear and month < firstdatemonth) or (year == cutoffyear and month > cutoffmonth):\n",
    "            continue\n",
    "        \n",
    "        if month == 2 and leapyear(year):\n",
    "            indexmonth = 0\n",
    "            \n",
    "        else:\n",
    "            indexmonth = month\n",
    "        \n",
    "        for day in range(1, days[indexmonth]+1):\n",
    "            if (year == firstdateyear and month == firstdatemonth and day < firstdateday) or (year == cutoffyear and month == cutoffmonth and day > cutoffday):\n",
    "                continue\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if count == 8:\n",
    "                count = 1\n",
    "                week += 1\n",
    "            \n",
    "            date = datejoin(day, month, year)\n",
    "            \n",
    "            datematchweek[date] = week\n",
    "\n",
    "dates = list(datematchweek.keys())\n",
    "weekno = list(datematchweek.values())\n",
    "\n",
    "# Make the dictionary of dates to week number into a dataframe\n",
    "DatesToWeek_DF = pd.DataFrame({'dates': dates, 'weekno':weekno})\n",
    "\n",
    "DatesToWeek_DF.to_csv('../DateMatchWeek.csv')\n",
    "\n",
    "# Record the week number of the cut-off date\n",
    "thisweek = max(weekno)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # PART 2: WRANGLING ORIGINAL DATA\n",
    "\n",
    "#### FUTURE UPDATE: towranglelist1 stores records which have dates in format [d]d/[m]m/yyyy; towranglelist2 stores records which have dates in format yyyy-mm-dd. Need to update lists accordingly\n",
    "towranglelist1 = ['action.csv', 'competency_record.csv', 'form_record.csv', 'incident.csv', 'users.csv']\n",
    "towranglelist2 = ['assets.csv', 'form_template.csv']\n",
    "towranglelist3 = ['users.csv'] \n",
    "towranglelist4 = ['users.csv']\n",
    "# because users need to be wrangled in terms of employees and inductioneers. 3 is inductionuser 4 is employees\n",
    "\n",
    "def wrangle(filename, datedata, mode):\n",
    "    \"\"\" cleans the file. 4 modes for four different ways to clean the data - all pretty similar except mode 3 and 4 selects users of particular hr types, and mode 2 deals with dates of a different format \"\"\"\n",
    "    data = pd.read_csv(f\"./Data/{filename}\")\n",
    "    domain = list(data['domain'])\n",
    "    \n",
    "    # First drop: get rid of rows from domains demo and demo_2\n",
    "    if mode in [1,2]:\n",
    "        droplist = []\n",
    "        for i in range(len(domain)):\n",
    "            if domain[i] in ['demo', 'demo_2', 'cruse']:\n",
    "                droplist.append(i)\n",
    "                \n",
    "    elif mode == 3:\n",
    "        domain = list(data['domain'])\n",
    "        hr = list(data['hr_type'])\n",
    "        \n",
    "        req = ['InductionUser']\n",
    "        \n",
    "        droplist = []\n",
    "        for i in range(len(domain)):\n",
    "            if domain[i] in ['demo', 'demo_2', 'cruse'] or hr[i] not in req:\n",
    "                droplist.append(i)\n",
    "    \n",
    "    else:\n",
    "        domain = list(data['domain'])\n",
    "        hr = list(data['hr_type'])\n",
    "        \n",
    "        req = ['Casual', 'Employee', 'Subcontractor']\n",
    "        \n",
    "        droplist = []\n",
    "        for i in range(len(domain)):\n",
    "            if domain[i] in ['demo', 'demo_2', 'cruse'] or hr[i] not in req:\n",
    "                droplist.append(i)\n",
    "\n",
    "    data = data.drop(droplist)\n",
    "    \n",
    "    # re-setup date dictionary from the DataFrame\n",
    "    dates = list(datedata['dates'])\n",
    "    weekno = list(datedata['weekno'])\n",
    "    datematchdict = dict()\n",
    "    for i in range(len(dates)):\n",
    "        datematchdict[dates[i]] = weekno[i]\n",
    "    \n",
    "    # Second drop: clean out rows whose dates are not within startdate and cutoffdate\n",
    "    #### FUTURE CHANGE: this step takes quite a lot of time - could be area to improve algorithmically\n",
    "    data.index = (range(0, len(list(data['created_at'])))) #re-do index after dropping demo and demo_2\n",
    "    actdate = list(data['created_at'])\n",
    "    \n",
    "    # If any data happens to have date in format \"dd-mm-yyyy\" then need to put file in towranglelist2. Else put in towranglelist1. Note dates should be in format \"[d]d/[m]m/yyyy\"\n",
    "    newdroplist = []\n",
    "    if mode == 2:\n",
    "        def transform_date(inputdate):\n",
    "            \"\"\" helper function to transform date in format of dd-mm-yyyy into [d]d/[m]m/yyyy which is what datedata produced in PART 1 stores  \"\"\"\n",
    "            splitted = inputdate.split('-')\n",
    "            if int(splitted[1]) < 10:\n",
    "                month = splitted[1][1]\n",
    "            else:\n",
    "                month = splitted[1]\n",
    "\n",
    "            if int(splitted[2]) < 10:\n",
    "                day = splitted[2][1]\n",
    "            else:\n",
    "                day = splitted[2]\n",
    "\n",
    "            return f'{day}/{month}/{splitted[0]}'\n",
    "        \n",
    "        for i in range(len(actdate)):\n",
    "            if transform_date(actdate[i].split()[0]) not in dates:\n",
    "                newdroplist.append(i)\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(actdate)):\n",
    "            if actdate[i].split()[0] not in dates:\n",
    "                newdroplist.append(i)\n",
    "        \n",
    "    data = data.drop(newdroplist) # drop the rows of data whose dates are not between startdate and cutoffdate\n",
    "    \n",
    "    actdate = list(data['created_at']) #reread the date created column now that we've dropped some rows\n",
    "    \n",
    "    newdomain = list(data['domain'])\n",
    "    # get a new list matching each action to the week that they were done in\n",
    "    actweekno = list()\n",
    "    \n",
    "    if mode == 2:\n",
    "        for i in range(len(actdate)):\n",
    "            actweekno.append(datematchdict[transform_date(actdate[i].split()[0])])\n",
    "    else:\n",
    "        for i in range(len(actdate)):\n",
    "            actweekno.append(datematchdict[actdate[i].split()[0]]) # use [0] because string also contains hour:minute:second\n",
    "    \n",
    "    # At this point, now have two lists newdomain and actweekno: in the former the ith value is the domain of the ith row, and the latter the ith value is the relative week since FIRSTSTARTDATE that the ith row was created in. Now just count them up \n",
    "    \n",
    "    # count up the numbers of actions this week by domain and week\n",
    "    groupup = dict()\n",
    "    for i in range(len(actweekno)):\n",
    "        if f'{newdomain[i]} {actweekno[i]}' in groupup:\n",
    "            groupup[f'{newdomain[i]} {actweekno[i]}'] += 1\n",
    "        else:\n",
    "            groupup[f'{newdomain[i]} {actweekno[i]}'] = 1\n",
    "            \n",
    "    groupupkey = list(groupup.keys())\n",
    "    groupupval = list(groupup.values())\n",
    "\n",
    "    # create lists that contain just domain name and week number\n",
    "    out1 = list()\n",
    "    out2 = list()\n",
    "\n",
    "    for i in range(len(groupupkey)):\n",
    "        out1.append(groupupkey[i].split()[0])\n",
    "        out2.append(groupupkey[i].split()[1])\n",
    "    \n",
    "    # export the wrangled file as a csv (each of these files are wrangled version of the raw data files (of each of the client's recorded activity in lucidity) in terms of counts per week per domain)\n",
    "    out = pd.DataFrame({'Domain': out1, 'Week': out2, 'COUNT': groupupval})\n",
    "    \n",
    "    if mode in [1,2]:\n",
    "        out.to_csv(f'./Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "        out.to_csv(f'../History/Week {thisweek}/Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "        \n",
    "    elif mode == 3:\n",
    "        out.to_csv('./Partial_Output/_2_Users_Inductee_clean.csv', index = False)\n",
    "        out.to_csv(f'../History/Week {thisweek}/Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "        \n",
    "    else:\n",
    "        out.to_csv('./Partial_Output/_2_Users_norm_employee_clean.csv', index = False)\n",
    "        out.to_csv(f'../History/Week {thisweek}/Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "\n",
    "# OS housekeeping and running each of the files through wrangle()\n",
    "if not os.path.exists('./Partial_Output'):\n",
    "    os.mkdir('./Partial_Output')\n",
    "\n",
    "if not os.path.exists(f'../History/Week {thisweek}/Partial_Output'):\n",
    "    os.makedirs(f'../History/Week {thisweek}/Partial_Output')\n",
    "        \n",
    "for file in towranglelist1:\n",
    "    wrangle(file, DatesToWeek_DF, 1)\n",
    "    \n",
    "for file in towranglelist2:\n",
    "    wrangle(file, DatesToWeek_DF, 2)\n",
    "    \n",
    "for file in towranglelist3:\n",
    "    wrangle(file, DatesToWeek_DF, 3)\n",
    "    \n",
    "for file in towranglelist4:\n",
    "    wrangle(file, DatesToWeek_DF, 4)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# PART 3: COMBINE PREVIOUSLY WRANGLED DATAFRAMES INTO ONE (FILLING IN WEEKS WITH NO ACTIVITY)\n",
    "\n",
    "# import all cleaned data\n",
    "asset = pd.read_csv('./Partial_Output/_2_assets_clean.csv')\n",
    "actions = pd.read_csv('./Partial_Output/_2_action_clean.csv')\n",
    "competency = pd.read_csv('./Partial_Output/_2_competency_record_clean.csv')\n",
    "form_record = pd.read_csv('./Partial_Output/_2_form_record_clean.csv')\n",
    "form_templates = pd.read_csv('./Partial_Output/_2_form_template_clean.csv')\n",
    "incidents = pd.read_csv('./Partial_Output/_2_incident_clean.csv')\n",
    "users = pd.read_csv('./Partial_Output/_2_users_clean.csv')\n",
    "users_induct = pd.read_csv('./Partial_Output/_2_users_Inductee_clean.csv')\n",
    "users_norm_emp = pd.read_csv('./Partial_Output/_2_users_norm_employee_clean.csv')\n",
    "\n",
    "# Find a set of the domain names - for finding the \"earliest recorded date\" of activity/usage for each\n",
    "set1 = set(asset['Domain'])\n",
    "set2 = set(actions['Domain'])\n",
    "set3 = set(competency['Domain'])\n",
    "set4 = set(form_record['Domain'])\n",
    "set5 = set(form_templates['Domain'])\n",
    "set6 = set(incidents['Domain'])\n",
    "set7 = set(users['Domain'])\n",
    "\n",
    "fullset = set1.union(set2).union(set3).union(set4).union(set5).union(set6).union(set7)\n",
    "fullsetlist = list(fullset) # Now have a full set of the domains\n",
    "fullsetlist.sort()\n",
    "\n",
    "iteration = [asset, actions, competency, form_record, form_templates, incidents, users]\n",
    "newiteration = [asset, actions, competency, form_record, form_templates, incidents, users, users_induct, users_norm_emp]\n",
    "\n",
    "# Find first week recorded and put them in a dictionary of (key: value) = (domain name: first week of activity)\n",
    "startweek = dict()\n",
    "\n",
    "for data in newiteration:\n",
    "    dom = list(data['Domain'])\n",
    "    week = list(data['Week'])\n",
    "    count = list(data['COUNT'])\n",
    "    \n",
    "    for i in range(len(dom)):\n",
    "        if dom[i] in startweek:\n",
    "            if week[i] < startweek[dom[i]]:\n",
    "                startweek[dom[i]] = week[i]\n",
    "        else:\n",
    "            startweek[dom[i]] = week[i]\n",
    "            \n",
    "startweeklist = list(startweek.items())\n",
    "startweeklist.sort()\n",
    "\n",
    "# Create a template for recording the data (now fill out gaps between start week and week 216 where there is 0 data)\n",
    "combineddatatemplate = dict()\n",
    "\n",
    "# first initiate a blank dictionary with all weeks from first week of activity to cutoffdate's week\n",
    "for i in range(len(startweeklist)):\n",
    "    for j in range(startweeklist[i][1], thisweek+1):\n",
    "        combineddatatemplate[f'{startweeklist[i][0]} {j}'] = 0\n",
    "\n",
    "# create blank copies of this initialised template dictionary, and fill them in based on counts from the output of PART 2\n",
    "assetcomb = combineddatatemplate.copy()\n",
    "actionscomb = combineddatatemplate.copy()\n",
    "competencycomb = combineddatatemplate.copy()\n",
    "form_recordcomb = combineddatatemplate.copy()\n",
    "form_templatescomb = combineddatatemplate.copy()\n",
    "incidentscomb = combineddatatemplate.copy()\n",
    "userscomb = combineddatatemplate.copy()\n",
    "users_inductcomb = combineddatatemplate.copy()\n",
    "users_norm_empcomb = combineddatatemplate.copy()\n",
    "\n",
    "dictlist = [assetcomb, actionscomb, competencycomb, form_recordcomb, form_templatescomb, incidentscomb, userscomb, users_inductcomb, users_norm_empcomb]\n",
    "\n",
    "# Now fill in the details where there are records (because all dictionary slots initialised, only need to repalce data for weeks where there was a count recorded, and all other are fine to be left untouched - just ends up being 0)\n",
    "for k in range(len(dictlist)):\n",
    "    dom = list(newiteration[k]['Domain'])\n",
    "    week = list(newiteration[k]['Week'])\n",
    "    count = list(newiteration[k]['COUNT'])\n",
    "    \n",
    "    for i in range(len(dom)):\n",
    "        dictlist[k][f'{dom[i]} {week[i]}'] = count[i]\n",
    "\n",
    "uniqueid = list(assetcomb.keys())\n",
    "assetcount = list(assetcomb.values())\n",
    "actioncount = list(actionscomb.values())\n",
    "competencycount = list(competencycomb.values())\n",
    "form_recordcount = list(form_recordcomb.values())\n",
    "form_templatescount = list(form_templatescomb.values())\n",
    "incidentscount = list(incidentscomb.values())\n",
    "userscount = list(userscomb.values())\n",
    "users_inductcount = list(users_inductcomb.values())\n",
    "users_norm_empcount = list(users_norm_empcomb.values())\n",
    "\n",
    "# create two more lists that contain just domain and just week - maximises chances of making future wrangling easier\n",
    "doms = []\n",
    "weekss = []\n",
    "\n",
    "for i in range(len(uniqueid)):\n",
    "    doms.append(uniqueid[i].split()[0])\n",
    "    weekss.append(uniqueid[i].split()[1])\n",
    "    \n",
    "# Create a new column for counting the number of weeks since particular company started at Lucidity\n",
    "selfweeks = []\n",
    "count = 0\n",
    "\n",
    "# (logic of loop fairly simple - if domain column runs into new company then reset the count)\n",
    "prev = doms[0]\n",
    "for i in range(len(doms)):\n",
    "    if doms[i] == prev:\n",
    "        count += 1\n",
    "        selfweeks.append(count)\n",
    "    else:\n",
    "        count = 1\n",
    "        selfweeks.append(count)\n",
    "    prev = doms[i]\n",
    "\n",
    "\n",
    "# turn it into one dataframe and output\n",
    "out = pd.DataFrame({'ID': uniqueid, 'Domain': doms, 'Week': weekss, 'Selfweeks': selfweeks,\n",
    "                    'Assets': assetcount, 'Actions': actioncount, 'Competency': competencycount, \n",
    "                    'Form_record': form_recordcount, 'Form_template': form_templatescount,\n",
    "                   'Incident': incidentscount, 'Users': userscount, \n",
    "                    'Users_induction': users_inductcount, 'Users_norm_emp': users_norm_empcount})\n",
    "\n",
    "# This file now has counts of all activities grouped by week by client/domain, sorted by domian and clients.  \n",
    "out.to_csv(\"./Partial_Output/_3_combined_cleaned_data.csv\", index = False)\n",
    "out.to_csv(f\"../History/Week {thisweek}/Partial_Output/_3_combined_cleaned_data.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278aeae2-c458-410b-b84b-d7d7d70cd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./PreparedData'):\n",
    "    os.mkdir('./PreparedData')\n",
    "\n",
    "if not os.path.exists('./SplitData'):\n",
    "    os.mkdir('./SplitData')\n",
    "\n",
    "    \n",
    "if not os.path.exists('../Standardisers'):\n",
    "    os.mkdir('../Standardisers')\n",
    "\n",
    "if not os.path.exists('../Models'):\n",
    "    os.mkdir('../Models')\n",
    "\n",
    "if not os.path.exists('../History/Week 215/PreparedData'):\n",
    "    os.mkdir('../History/Week 215/PreparedData')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11c308c-b538-4034-8733-373e3d83b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipdata(data, NWEEKS, attrb, meth, maniptype, discard):\n",
    "    \n",
    "    metadata = ['ID', 'Domain', 'Week', 'Selfweeks']\n",
    "    masterlist = [list() for i in range(NWEEKS+5)]\n",
    "    \n",
    "    skip = 0\n",
    "    if discard:\n",
    "        skip = 26\n",
    "    \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i]['Selfweeks'] > NWEEKS+skip and i+1 < len(data) and data.loc[i]['Domain'] == data.loc[i+1]['Domain']:\n",
    "            \n",
    "            for j in range(len(metadata)):\n",
    "                masterlist[j].append(data.loc[i][metadata[j]])\n",
    "                \n",
    "            for j in range(4, NWEEKS+4):\n",
    "                masterlist[j].append(data.loc[i-(j-4)][attrb])\n",
    "            \n",
    "            masterlist[NWEEKS+4].append(data.loc[i][f'{attrb}T'])\n",
    "    \n",
    "    out = pd.DataFrame()\n",
    "        \n",
    "    for i in range(NWEEKS+5):\n",
    "        if i < 4:\n",
    "            out.insert(i, metadata[i], masterlist[i])\n",
    "        elif i == NWEEKS + 4:\n",
    "            out.insert(i, 'Target', masterlist[i])\n",
    "        else:\n",
    "            out.insert(i, f'{i-4}', masterlist[i])\n",
    "    \n",
    "    out.to_csv(f'./PreparedData/{maniptype}_{meth}_{attrb}_{NWEEKS}.csv', index = False)\n",
    "    out.to_csv(f'../History/Week 215/PreparedData/{maniptype}_{meth}_{attrb}_{NWEEKS}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60fe5a6-2e7f-4191-8a11-84b44e5ed6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "meth = '-26'\n",
    "\n",
    "df = pd.read_csv('./Partial_Output/_3_combined_cleaned_data.csv')\n",
    "df = df[df.columns[:-1]]\n",
    "\n",
    "discard = False\n",
    "\n",
    "if '-26' in meth:\n",
    "    df = df[df['Selfweeks'] > 26]\n",
    "    df.index = range(len(df))\n",
    "\n",
    "    discard = True\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for domain, tmp in df.groupby('Domain'):\n",
    "    tmp.index = range(len(tmp))\n",
    "\n",
    "    for i in range(len(tmp)-1, -1, -1):\n",
    "        if ((tmp.loc[i]['Assets']) | (tmp.loc[i]['Actions']) | (tmp.loc[i]['Competency']) |\n",
    "                (tmp.loc[i]['Form_record']) | (tmp.loc[i]['Form_template']) |\n",
    "                (tmp.loc[i]['Incident']) | (tmp.loc[i]['Users'])):\n",
    "            break\n",
    "\n",
    "    data = pd.concat([data, tmp[0:i+1]])\n",
    "\n",
    "    \n",
    "stdData = pd.DataFrame()\n",
    "\n",
    "for domain, compData in data.groupby('Domain'):\n",
    "\n",
    "    compDataX = compData[['Assets', 'Actions', 'Competency',\n",
    "       'Form_record', 'Form_template', 'Incident', 'Users']]\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(compDataX)\n",
    "\n",
    "    with open(f'../Standardisers/{domain}.pickle', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    compData[['Assets', 'Actions', 'Competency',\n",
    "       'Form_record', 'Form_template', 'Incident', 'Users']] = scaler.transform(compDataX)\n",
    "\n",
    "    stdData = pd.concat([stdData, compData])\n",
    "\n",
    "for colName in stdData.columns[4:]:\n",
    "\n",
    "    target = list()\n",
    "\n",
    "    for domain, compData in stdData.groupby('Domain'):\n",
    "\n",
    "        compData.index = compData['Selfweeks']\n",
    "\n",
    "        index = compData.index\n",
    "\n",
    "        out = [(compData.loc[i+1][colName]-compData.loc[i][colName]) if (i+1 in index)\n",
    "               else np.nan for i in index]\n",
    "        target.extend(out)\n",
    "\n",
    "    stdData[f'{colName}T'] = target\n",
    "\n",
    "stdData['Week'].astype(int)\n",
    "stdData = stdData.sort_values(['Domain', 'Week'])\n",
    "stdData.index = range(len(stdData))\n",
    "\n",
    "for attrb in stdData.columns[4:11]:\n",
    "    for nWeeks in [11]:\n",
    "        manipdata(stdData, nWeeks, attrb, meth, 'S_D', discard)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6748d2-2ca4-4c44-a8b3-70685e0f4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = os.listdir(\"./PreparedData\")\n",
    "filelist.sort()\n",
    "for file in filelist:\n",
    "    if file[0] != '.':\n",
    "        stdData = pd.read_csv(f\"./PreparedData/{file}\")\n",
    "\n",
    "        trainWeeks, testWeeks = train_test_split(range(12, 215), train_size = 0.8, test_size = 0.2, random_state = 42)\n",
    "        testBool = stdData.Week.isin(testWeeks)\n",
    "        testData = stdData[testBool]\n",
    "\n",
    "        trainBool = stdData.Week.isin(trainWeeks)\n",
    "        trainData = stdData[trainBool]\n",
    "\n",
    "        testData.to_csv(f'./SplitData/{file.strip(\".csv\")}_Test.csv', index = False)\n",
    "        trainData.to_csv(f'./SplitData/{file.strip(\".csv\")}_Train.csv', index = False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e81269-27f9-4d71-9ad3-41dd2c6cd303",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Assets', 'Actions', 'Competency',\n",
    "       'Form_record', 'Form_template', 'Incident', 'Users']\n",
    "\n",
    "fsval = 0.4\n",
    "r = 'r2'\n",
    "drop = '-26'\n",
    "nweeks = 11\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "models = dict()\n",
    "\n",
    "for attb in cols:   \n",
    "    df = pd.read_csv(f'./SplitData/S_D_{drop}_{attb}_{nweeks}_Train.csv')\n",
    "\n",
    "    x = df[df.columns[4:-1]]\n",
    "    y = df[df.columns[-1]]\n",
    "\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(x, y)\n",
    "\n",
    "    if lm.score(x, y) > fsval:\n",
    "        \n",
    "        with open(f'../Models/{attb}.pickle', 'wb') as f:\n",
    "            pickle.dump([model, lm.score(x, y)], f)\n",
    "\n",
    "        models[attb] = (model, lm.score(x, y))\n",
    "\n",
    "with open(f'../Models/attbs.pickle', 'wb') as f:\n",
    "    pickle.dump(list(models.keys()), f)\n",
    "\n",
    "# Test data\n",
    "test = pd.read_csv(f'./SplitData/S_D_{drop}_{attb}_{nweeks}_Test.csv')\n",
    "out = test[test.columns[0:4]]\n",
    "\n",
    "for col in models.keys():\n",
    "    test = pd.read_csv(f'./SplitData/S_D_{drop}_{attb}_{nweeks}_Test.csv')\n",
    "\n",
    "    xTest = test[test.columns[4:-1]]\n",
    "    yTest = test[test.columns[-1]]\n",
    "\n",
    "    lm = models[col][0]\n",
    "\n",
    "    yPred = lm.predict(xTest)\n",
    "\n",
    "    out[f'{col}O'] = yTest \n",
    "    out[f'{col}P'] = yPred\n",
    "\n",
    "out['ObsScore'] = [0 for i in range(len(out))]\n",
    "out['PredScore'] = [0 for i in range(len(out))]\n",
    "\n",
    "if r == 'r':\n",
    "    for col in models.keys():\n",
    "        out['ObsScore'] = out['ObsScore'] + models[col][1]*out[f'{col}O']\n",
    "        out['PredScore'] = out['PredScore'] + models[col][1]*out[f'{col}P']\n",
    "\n",
    "elif r == 'r2':\n",
    "    for col in models.keys():\n",
    "        out['ObsScore'] = out['ObsScore'] + ((models[col][1])**2)*out[f'{col}O']\n",
    "        out['PredScore'] = out['PredScore'] + ((models[col][1])**2)*out[f'{col}P']\n",
    "else:\n",
    "    for col in models.keys():\n",
    "        out['ObsScore'] = out['ObsScore'] + out[f'{col}O']\n",
    "        out['PredScore'] = out['PredScore'] + out[f'{col}P']\n",
    "\n",
    "final = pd.DataFrame()\n",
    "for week, data in out.groupby(['Week']):\n",
    "    data.index = range(len(data))\n",
    "    O95 = np.quantile(data['ObsScore'], .95)\n",
    "    O05 = np.quantile(data['ObsScore'], .05)\n",
    "\n",
    "    P95 = np.quantile(data['PredScore'], .95)\n",
    "    P05 = np.quantile(data['PredScore'], .05)\n",
    "\n",
    "    data['Obs'] = ['Increase' if data.loc[i]['ObsScore'] > O95 else 'Decrease' if data.loc[i]['ObsScore'] < O05 else 'Normal' for i in range(len(data))]\n",
    "    data['Pred'] = ['Increase' if data.loc[i]['PredScore'] > P95 else 'Decrease' if data.loc[i]['PredScore'] < P05 else 'Normal' for i in range(len(data))]\n",
    "\n",
    "    final = pd.concat([final, data])\n",
    "\n",
    "n1 = len(final[final['Pred'] == 'Increase'])\n",
    "n2 = len(final[final['Pred'] == 'Decrease'])\n",
    "o1 = len(final[final['Obs'] == 'Increase'])\n",
    "o2 = len(final[final['Obs'] == 'Decrease'])\n",
    "    \n",
    "tp1 = len(final[(final['Pred'] == 'Increase') & (final['Obs'] == final['Pred'])])/n1\n",
    "tp2 = len(final[(final['Pred'] == 'Decrease') & (final['Obs'] == final['Pred'])])/n2\n",
    "bfp1 = len(final[(final['Obs'] == 'Decrease') & (final['Pred'] == 'Increase')])/n1\n",
    "bfp2 = len(final[(final['Obs'] == 'Increase') & (final['Pred'] == 'Decrease')])/n2\n",
    "s1 = len(final[(final['Obs'] == 'Increase') & (final['Obs'] == final['Pred'])])/o1\n",
    "s2 = len(final[(final['Obs'] == 'Decrease') & (final['Obs'] == final['Pred'])])/o2\n",
    "\n",
    "\n",
    "CentralStatisticCol = {'TargetType': ['S'], 'TargetMeth': ['D'], 'Method': [drop], 'NWEEKS': [nweeks],\n",
    "         'r2': [r], 'fsval': [fsval], 'TP': [(tp1 + tp2)/2], 'BFP': [(bfp1+bfp2)/2], \n",
    "        'Sens': [(s1+s2)/2], 'TP1': [tp1], 'TP2': [tp2], 'BFP1': [bfp1], 'BFP2': [bfp2], 'Sens1': [s1], 'Sens2': [s2]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd60fe2-f801-4c2b-8872-5339661c2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics = pd.DataFrame({'Index':['True Positive 1', 'True Positive 2', 'Bad False Positive 1', 'Bad False Positive 2'], \n",
    "                           'Statistics': [tp1, tp2, bfp1, bfp2]})\n",
    "\n",
    "Statistics.index = Statistics.Index\n",
    "Statistics.to_csv('./Statistics.csv')\n",
    "Statistics.to_csv('../Statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa8a66-6265-4edf-a46a-cfa267d4c41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
