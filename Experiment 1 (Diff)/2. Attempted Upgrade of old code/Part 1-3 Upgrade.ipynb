{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a95f0b3-3d6d-474e-abce-83ee8d58bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 0: IMPORTING LIBRARIES\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math as m\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "598c29e5-8cb4-4176-a19b-565e0513f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: CREATE DATE FILE\n",
    "\n",
    "# Reads in 'cut-off date' for data to be used to train \n",
    "# currdate = sys.argv[1]\n",
    "currdate = pd.to_datetime('15/8/2021', dayfirst = True)\n",
    "iterdate = pd.to_datetime('03/07/2017', dayfirst = True)\n",
    "FIRSTDATE = pd.to_datetime('03/07/2017', dayfirst = True)\n",
    "\n",
    "\n",
    "day = datetime.timedelta(days=1)\n",
    "\n",
    "datematchweek = dict()\n",
    "week = 1\n",
    "count = 1\n",
    "while iterdate <= currdate:\n",
    "    if count == 8:\n",
    "        count = 1\n",
    "        week += 1\n",
    "    \n",
    "    datematchweek[iterdate.strftime('%d/%m/%Y')] = week\n",
    "    iterdate += day\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "dates = list(datematchweek.keys())\n",
    "weekno = list(datematchweek.values())\n",
    "\n",
    "# Make the dictionary of dates to week number into a dataframe\n",
    "DatesToWeek_DF = pd.DataFrame({'dates': dates, 'weekno':weekno})\n",
    "\n",
    "DatesToWeek_DF.to_csv('../DateMatchWeek.csv', index = False)\n",
    "\n",
    "# Record the week number of the cut-off date\n",
    "thisweek = max(weekno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f72d49ff-43b7-4667-bf93-48059cb44909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:78: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:78: DtypeWarning: Columns (12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:78: DtypeWarning: Columns (5,6,7,8,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:81: DtypeWarning: Columns (5,6,7,8,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:84: DtypeWarning: Columns (5,6,7,8,16) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 23s, sys: 4.66 s, total: 18min 28s\n",
      "Wall time: 18min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # PART 2: WRANGLING ORIGINAL DATA\n",
    "\n",
    "#### FUTURE UPDATE: towranglelist1 stores records which have dates in format [d]d/[m]m/yyyy; towranglelist2 stores records which have dates in format yyyy-mm-dd. Need to update lists accordingly\n",
    "towranglelist1 = ['action.csv', 'competency_record.csv', 'form_record.csv', 'incident.csv', 'users.csv','assets.csv', 'form_template.csv']\n",
    "towranglelist2 = ['users.csv'] \n",
    "# because users need to be wrangled in terms of employees and inductioneers. 3 is inductionuser 4 is employees\n",
    "\n",
    "def wrangle(filename, datedata, mode):\n",
    "    \"\"\" cleans the file. 4 modes for four different ways to clean the data - all pretty similar except mode 3 and 4 selects users of particular hr types, and mode 2 deals with dates of a different format \"\"\"\n",
    "    data = pd.read_csv(f\"./Data/{filename}\")\n",
    "    data = data[(data['domain'] != 'cruse') & (data['domain'] != 'demo') & (data['domain'] != 'demo_2')]\n",
    "    \n",
    "    # First drop: get rid of rows from domains demo and demo_2       \n",
    "    if mode == 2:\n",
    "        data = data[data['hr_type'] != 'InductionUser']\n",
    "        req = ['InductionUser']\n",
    "    \n",
    "    elif mode == 3:\n",
    "        data = data[(data['hr_type'] != 'Casual') & (data['hr_type'] != 'Employee') & (data['hr_type'] != 'Subcontractor')]\n",
    "    \n",
    "    # re-setup date dictionary from the DataFrame\n",
    "    dates = list(datedata['dates'])\n",
    "    weekno = list(datedata['weekno'])\n",
    "    \n",
    "    datematchdict = {dates[i]: weekno[i] for i in range(len(dates))}\n",
    "    \n",
    "    data['created_at'] = pd.to_datetime(data['created_at'], errors = 'coerce', dayfirst = True)\n",
    "    \n",
    "    data = data[(data['created_at'] >= FIRSTDATE) & (data['created_at'] <= currdate)]\n",
    "    \n",
    "    data['created_at'] = pd.to_datetime(data['created_at'], dayfirst = True)\n",
    "    \n",
    "    data.index = range(len(data))\n",
    "    \n",
    "    data['created_at'] = data['created_at'].dt.strftime('%d/%m/%Y')\n",
    "    \n",
    "    actweekno = [datematchdict[data.loc[i]['created_at']] for i in range(len(data))]\n",
    "    \n",
    "    \n",
    "    groupup = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        groupup[f\"{data.loc[i]['domain']} {actweekno[i]}\"] += 1\n",
    "    \n",
    "    groupupkey = list(groupup.keys())\n",
    "    groupupval = list(groupup.values())\n",
    "                 \n",
    "    # create lists that contain just domain name and week number\n",
    "    out1 = [groupupkey[i].split()[0] for i in range(len(groupupkey))]\n",
    "    out2 = [groupupkey[i].split()[1] for i in range(len(groupupkey))]\n",
    "    \n",
    "    # export the wrangled file as a csv (each of these files are wrangled version of the raw data files (of each of the client's recorded activity in lucidity) in terms of counts per week per domain)\n",
    "    out = pd.DataFrame({'Domain': out1, 'Week': out2, 'COUNT': groupupval})\n",
    "    \n",
    "    if mode == 1:\n",
    "        out.to_csv(f'./Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "        out.to_csv(f'../History/Week {thisweek}/Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "        \n",
    "    elif mode == 2:\n",
    "        out.to_csv('./Partial_Output/_2_Users_Inductee_clean.csv', index = False)\n",
    "        out.to_csv(f'../History/Week {thisweek}/Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "        \n",
    "    else:\n",
    "        out.to_csv('./Partial_Output/_2_Users_norm_employee_clean.csv', index = False)\n",
    "        out.to_csv(f'../History/Week {thisweek}/Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "    \n",
    "# OS housekeeping and running each of the files through wrangle()\n",
    "if not os.path.exists('./Partial_Output'):\n",
    "    os.mkdir('./Partial_Output')\n",
    "\n",
    "if not os.path.exists(f'../History/Week {thisweek}/Partial_Output'):\n",
    "    os.makedirs(f'../History/Week {thisweek}/Partial_Output')\n",
    "        \n",
    "for file in towranglelist1:\n",
    "    wrangle(file, DatesToWeek_DF, 1)\n",
    "    \n",
    "for file in towranglelist2:\n",
    "    wrangle(file, DatesToWeek_DF, 2)\n",
    "    \n",
    "for file in towranglelist2:\n",
    "    wrangle(file, DatesToWeek_DF, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c6042-f67e-4d54-b305-48018ae3a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: COMBINE PREVIOUSLY WRANGLED DATAFRAMES INTO ONE (FILLING IN WEEKS WITH NO ACTIVITY)\n",
    "\n",
    "# import all cleaned data\n",
    "asset = pd.read_csv('./Partial_Output/_2_assets_clean.csv')\n",
    "actions = pd.read_csv('./Partial_Output/_2_action_clean.csv')\n",
    "competency = pd.read_csv('./Partial_Output/_2_competency_record_clean.csv')\n",
    "form_record = pd.read_csv('./Partial_Output/_2_form_record_clean.csv')\n",
    "form_templates = pd.read_csv('./Partial_Output/_2_form_template_clean.csv')\n",
    "incidents = pd.read_csv('./Partial_Output/_2_incident_clean.csv')\n",
    "users = pd.read_csv('./Partial_Output/_2_users_clean.csv')\n",
    "users_induct = pd.read_csv('./Partial_Output/_2_users_Inductee_clean.csv')\n",
    "users_norm_emp = pd.read_csv('./Partial_Output/_2_users_norm_employee_clean.csv')\n",
    "\n",
    "# Find a set of the domain names - for finding the \"earliest recorded date\" of activity/usage for each\n",
    "set1 = set(asset['Domain'])\n",
    "set2 = set(actions['Domain'])\n",
    "set3 = set(competency['Domain'])\n",
    "set4 = set(form_record['Domain'])\n",
    "set5 = set(form_templates['Domain'])\n",
    "set6 = set(incidents['Domain'])\n",
    "set7 = set(users['Domain'])\n",
    "\n",
    "fullset = set1.union(set2).union(set3).union(set4).union(set5).union(set6).union(set7)\n",
    "fullsetlist = list(fullset) # Now have a full set of the domains\n",
    "fullsetlist.sort()\n",
    "\n",
    "iteration = [asset, actions, competency, form_record, form_templates, incidents, users]\n",
    "newiteration = [asset, actions, competency, form_record, form_templates, incidents, users, users_induct, users_norm_emp]\n",
    "\n",
    "# Find first week recorded and put them in a dictionary of (key: value) = (domain name: first week of activity)\n",
    "startweek = dict()\n",
    "\n",
    "for data in newiteration:\n",
    "    dom = list(data['Domain'])\n",
    "    week = list(data['Week'])\n",
    "    count = list(data['COUNT'])\n",
    "    \n",
    "    for i in range(len(dom)):\n",
    "        if dom[i] in startweek:\n",
    "            if week[i] < startweek[dom[i]]:\n",
    "                startweek[dom[i]] = week[i]\n",
    "        else:\n",
    "            startweek[dom[i]] = week[i]\n",
    "            \n",
    "startweeklist = list(startweek.items())\n",
    "startweeklist.sort()\n",
    "\n",
    "# Create a template for recording the data (now fill out gaps between start week and week 216 where there is 0 data)\n",
    "combineddatatemplate = dict()\n",
    "\n",
    "# first initiate a blank dictionary with all weeks from first week of activity to cutoffdate's week\n",
    "for i in range(len(startweeklist)):\n",
    "    for j in range(startweeklist[i][1], thisweek+1):\n",
    "        combineddatatemplate[f'{startweeklist[i][0]} {j}'] = 0\n",
    "\n",
    "# create blank copies of this initialised template dictionary, and fill them in based on counts from the output of PART 2\n",
    "assetcomb = combineddatatemplate.copy()\n",
    "actionscomb = combineddatatemplate.copy()\n",
    "competencycomb = combineddatatemplate.copy()\n",
    "form_recordcomb = combineddatatemplate.copy()\n",
    "form_templatescomb = combineddatatemplate.copy()\n",
    "incidentscomb = combineddatatemplate.copy()\n",
    "userscomb = combineddatatemplate.copy()\n",
    "users_inductcomb = combineddatatemplate.copy()\n",
    "users_norm_empcomb = combineddatatemplate.copy()\n",
    "\n",
    "dictlist = [assetcomb, actionscomb, competencycomb, form_recordcomb, form_templatescomb, incidentscomb, userscomb, users_inductcomb, users_norm_empcomb]\n",
    "\n",
    "# Now fill in the details where there are records (because all dictionary slots initialised, only need to repalce data for weeks where there was a count recorded, and all other are fine to be left untouched - just ends up being 0)\n",
    "for k in range(len(dictlist)):\n",
    "    dom = list(newiteration[k]['Domain'])\n",
    "    week = list(newiteration[k]['Week'])\n",
    "    count = list(newiteration[k]['COUNT'])\n",
    "    \n",
    "    for i in range(len(dom)):\n",
    "        dictlist[k][f'{dom[i]} {week[i]}'] = count[i]\n",
    "\n",
    "uniqueid = list(assetcomb.keys())\n",
    "assetcount = list(assetcomb.values())\n",
    "actioncount = list(actionscomb.values())\n",
    "competencycount = list(competencycomb.values())\n",
    "form_recordcount = list(form_recordcomb.values())\n",
    "form_templatescount = list(form_templatescomb.values())\n",
    "incidentscount = list(incidentscomb.values())\n",
    "userscount = list(userscomb.values())\n",
    "users_inductcount = list(users_inductcomb.values())\n",
    "users_norm_empcount = list(users_norm_empcomb.values())\n",
    "\n",
    "# create two more lists that contain just domain and just week - maximises chances of making future wrangling easier\n",
    "doms = []\n",
    "weekss = []\n",
    "\n",
    "for i in range(len(uniqueid)):\n",
    "    doms.append(uniqueid[i].split()[0])\n",
    "    weekss.append(uniqueid[i].split()[1])\n",
    "    \n",
    "# Create a new column for counting the number of weeks since particular company started at Lucidity\n",
    "selfweeks = []\n",
    "count = 0\n",
    "\n",
    "# (logic of loop fairly simple - if domain column runs into new company then reset the count)\n",
    "prev = doms[0]\n",
    "for i in range(len(doms)):\n",
    "    if doms[i] == prev:\n",
    "        count += 1\n",
    "        selfweeks.append(count)\n",
    "    else:\n",
    "        count = 1\n",
    "        selfweeks.append(count)\n",
    "    prev = doms[i]\n",
    "\n",
    "# Create a prelim score of sum of the three attributes to be used in determining a function\n",
    "baseactscore1 = list()\n",
    "for i in range(len(uniqueid)):\n",
    "    baseactscore1.append(form_recordcount[i]+competencycount[i]+users_inductcount[i])\n",
    "\n",
    "# turn it into one dataframe and output\n",
    "out = pd.DataFrame({'ID': uniqueid, 'Domain': doms, 'Week': weekss, 'Selfweeks': selfweeks,\n",
    "                    'Assets': assetcount, 'Actions': actioncount, 'Competency': competencycount, \n",
    "                    'Form_record': form_recordcount, 'Form_template': form_templatescount,\n",
    "                   'Incident': incidentscount, 'Users': userscount, \n",
    "                    'Users_induction': users_inductcount, 'Users_norm_emp': users_norm_empcount, \n",
    "                    \"Prelim_action_score\": baseactscore1})\n",
    "\n",
    "# This file now has counts of all activities grouped by week by client/domain, sorted by domian and clients.  \n",
    "out.to_csv(\"./Partial_Output/_3_combined_cleaned_data.csv\", index = False)\n",
    "out.to_csv(f\"../History/Week {thisweek}/Partial_Output/_3_combined_cleaned_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dc5f8-8da5-4941-b54d-e076b16dd064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
