{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62207fd2-d54c-4978-b02c-47b4748db669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 0: IMPORTING LIBRARIES\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math as m\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PART 1: CREATE DATE FILE\n",
    "# Slightly different from PART 1 of InitialTraining.py's part 1 because it also needs to account for a subset of dates (from just the date of argument 1 to the date of argument 2)\n",
    "\n",
    "# Reads in 'cut-off date' for data to be used to train \n",
    "# datastartdate = sys.argv[1]\n",
    "# currdate = sys.argv[2]\n",
    "\n",
    "datastartdate = '30/8/2021'\n",
    "currdate = '5/9/2021'\n",
    "\n",
    "# Assumes that date format in 'dd/mm/yyyy' format\n",
    "cutoffday = int(currdate.split('/')[0])\n",
    "cutoffmonth = int(currdate.split('/')[1])\n",
    "cutoffyear = int(currdate.split('/')[2])\n",
    "\n",
    "def datejoin(day, month, year):\n",
    "    \"\"\"For joining up three numbers into a date\"\"\"\n",
    "    return (f'{str(day)}/{str(month)}/{str(year)}')\n",
    "\n",
    "\n",
    "def leapyear(year):\n",
    "    \"\"\"For determining whether a year is a leap year\"\"\"\n",
    "    if year % 4 == 0:\n",
    "        if year% 100 == 0:\n",
    "            if year%400 == 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Creates a dictionary matching each day to a week number (counting Week of July 3rd 2021 as Week 1)    \n",
    "#### FUTURE CHANGE: if wish to include data earlier than Monday July 3rd 2017, change the magic string FIRSTDATE\n",
    "FIRSTDATE = '03/07/2017'\n",
    "firstdateday = int(FIRSTDATE.split('/')[0])\n",
    "firstdatemonth = int(FIRSTDATE.split('/')[1])\n",
    "firstdateyear = int(FIRSTDATE.split('/')[2])\n",
    "\n",
    "datastartdateday = int(datastartdate.split('/')[0])\n",
    "datastartdatemonth = int(datastartdate.split('/')[1])\n",
    "datastartdateyear = int(datastartdate.split('/')[2])\n",
    "\n",
    "days = [29, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]    \n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "years = range(2017, cutoffyear+1)\n",
    "\n",
    "\n",
    "datematchweek = dict()\n",
    "minidatematchweek = dict()\n",
    "week = 1\n",
    "count = 0\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        \n",
    "        if (year == firstdateyear and month < firstdatemonth) or (year == cutoffyear and month > cutoffmonth):\n",
    "            continue\n",
    "        \n",
    "        if month == 2 and leapyear(year):\n",
    "            indexmonth = 0\n",
    "            \n",
    "        else:\n",
    "            indexmonth = month\n",
    "        \n",
    "        for day in range(1, days[indexmonth]+1):\n",
    "            if (year == firstdateyear and month == firstdatemonth and day < firstdateday) or (year == cutoffyear and month == cutoffmonth and day > cutoffday):\n",
    "                continue\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if count == 8:\n",
    "                count = 1\n",
    "                week += 1\n",
    "            \n",
    "            date = datejoin(day, month, year)\n",
    "            \n",
    "            datematchweek[date] = week\n",
    "            \n",
    "            if year > datastartdateyear:\n",
    "                \n",
    "                minidatematchweek[date] = week\n",
    "                \n",
    "            elif year == datastartdateyear:\n",
    "                \n",
    "                if month > datastartdatemonth:\n",
    "                    \n",
    "                    minidatematchweek[date] = week\n",
    "                    \n",
    "                elif month == datastartdatemonth and day >= datastartdateday:  \n",
    "                            \n",
    "                    minidatematchweek[date] = week\n",
    "\n",
    "dates = list(datematchweek.keys())\n",
    "weekno = list(datematchweek.values())\n",
    "\n",
    "# Make the dictionary of dates to week number into a dataframe\n",
    "DatesToWeek_DF = pd.DataFrame({'dates': dates, 'weekno':weekno})\n",
    "\n",
    "DatesToWeek_DF.to_csv('DateMatchWeek.csv')\n",
    "\n",
    "# Record the week number of the cut-off date\n",
    "thisweek = max(weekno)\n",
    "\n",
    "# create a subset just for wrangling this week's data \n",
    "minidates = list(minidatematchweek.keys())\n",
    "miniweekno = list(minidatematchweek.values())\n",
    "\n",
    "MiniDatesToWeek_DF = pd.DataFrame({'dates': minidates, 'weekno':miniweekno})\n",
    "\n",
    "datastartweek = min(miniweekno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc97f15-7a20-4559-bacb-cc304be2cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If has already run ContinuousTrain.py with same arguments then this PART 2 and PART 3 do not need to be rerun\n",
    "if not os.path.isfile(f'./History/Week {thisweek}/_3_combined_cleaned_data.csv'):\n",
    "    # PART 2: WRANGLING ORIGINAL DATA\n",
    "\n",
    "    #### FUTURE UPDATE: towranglelist1 stores records which have dates in format [d]d/[m]m/yyyy; towranglelist2 stores records which have dates in format yyyy-mm-dd. Need to update lists accordingly\n",
    "    towranglelist1 = [] #### FUTURE UPDATE: because all data at the time of writing came from Quicksight, all their formats were yyyy-mm-dd so all were put into towranglelist2\n",
    "    towranglelist2 = ['action.csv', 'competency_record.csv', 'form_record.csv', 'incident.csv', 'users.csv', 'assets.csv', 'form_template.csv']\n",
    "    towranglelist3 = ['users.csv'] \n",
    "    towranglelist4 = ['users.csv']\n",
    "    # because users need to be wrangled in terms of employees and inductioneers. 3 is inductionuser 4 is employees\n",
    "\n",
    "    def wrangle(filename, datedata, mode):\n",
    "        \"\"\" cleans the file. 4 modes for four different ways to clean the data - all pretty similar except mode 3 and 4 selects users of particular hr types, and mode 2 deals with dates of a different format \"\"\"\n",
    "        data = pd.read_csv(f\"./History/Week {thisweek}/Data/{filename}\")\n",
    "        domain = list(data['domain'])\n",
    "\n",
    "        # First drop: get rid of rows from domains demo and demo_2\n",
    "        if mode in [1,2]:\n",
    "            droplist = []\n",
    "            for i in range(len(domain)):\n",
    "                if domain[i] in ['demo', 'demo_2', 'cruse']:\n",
    "                    droplist.append(i)\n",
    "\n",
    "        elif mode == 3:\n",
    "            domain = list(data['domain'])\n",
    "            hr = list(data['hr_type'])\n",
    "\n",
    "            req = ['InductionUser']\n",
    "\n",
    "            droplist = []\n",
    "            for i in range(len(domain)):\n",
    "                if domain[i] in ['demo', 'demo_2', 'cruse'] or hr[i] not in req:\n",
    "                    droplist.append(i)\n",
    "\n",
    "        else:\n",
    "            domain = list(data['domain'])\n",
    "            hr = list(data['hr_type'])\n",
    "\n",
    "            req = ['Casual', 'Employee', 'Subcontractor']\n",
    "\n",
    "            droplist = []\n",
    "            for i in range(len(domain)):\n",
    "                if domain[i] in ['demo', 'demo_2', 'cruse'] or hr[i] not in req:\n",
    "                    droplist.append(i)\n",
    "\n",
    "        data = data.drop(droplist)\n",
    "\n",
    "        # re-setup date dictionary from the DataFrame\n",
    "        dates = list(datedata['dates'])\n",
    "        weekno = list(datedata['weekno'])\n",
    "        datematchdict = dict()\n",
    "        for i in range(len(dates)):\n",
    "            datematchdict[dates[i]] = weekno[i]\n",
    "\n",
    "        # Second drop: clean out rows whose dates are not within startdate and cutoffdate\n",
    "        #### FUTURE CHANGE: this step takes quite a lot of time - could be area to improve algorithmically\n",
    "        data.index = (range(0, len(list(data['created_at'])))) #re-do index after dropping demo and demo_2\n",
    "        actdate = list(data['created_at'])\n",
    "\n",
    "        # If any data happens to have date in format \"dd-mm-yyyy\" then need to put file in towranglelist2. Else put in towranglelist1. Note dates should be in format \"[d]d/[m]m/yyyy\"\n",
    "        newdroplist = []\n",
    "        if mode in [2,3,4]:\n",
    "            def transform_date(inputdate):\n",
    "                \"\"\" helper function to transform date in format of dd-mm-yyyy into [d]d/[m]m/yyyy which is what datedata produced in PART 1 stores  \"\"\"\n",
    "                splitted = inputdate.split('-')\n",
    "                if int(splitted[1]) < 10:\n",
    "                    month = splitted[1][1]\n",
    "                else:\n",
    "                    month = splitted[1]\n",
    "\n",
    "                if int(splitted[2]) < 10:\n",
    "                    day = splitted[2][1]\n",
    "                else:\n",
    "                    day = splitted[2]\n",
    "\n",
    "                return f'{day}/{month}/{splitted[0]}'\n",
    "\n",
    "            for i in range(len(actdate)):\n",
    "                if transform_date(actdate[i].split()[0]) not in dates:\n",
    "                    newdroplist.append(i)\n",
    "\n",
    "        else:\n",
    "            for i in range(len(actdate)):\n",
    "                if actdate[i].split()[0] not in dates:\n",
    "                    newdroplist.append(i)\n",
    "\n",
    "        data = data.drop(newdroplist) # drop the rows of data whose dates are not between startdate and cutoffdate\n",
    "\n",
    "        actdate = list(data['created_at']) #reread the date created column now that we've dropped some rows\n",
    "\n",
    "        newdomain = list(data['domain'])\n",
    "        # get a new list matching each action to the week that they were done in\n",
    "        actweekno = list()\n",
    "\n",
    "        if mode in [2, 3, 4]:\n",
    "            for i in range(len(actdate)):\n",
    "                actweekno.append(datematchdict[transform_date(actdate[i].split()[0])])\n",
    "        else:\n",
    "            for i in range(len(actdate)):\n",
    "                actweekno.append(datematchdict[actdate[i].split()[0]]) # use [0] because string also contains hour:minute:second\n",
    "\n",
    "        # At this point, now have two lists newdomain and actweekno: in the former the ith value is the domain of the ith row, and the latter the ith value is the relative week since FIRSTSTARTDATE that the ith row was created in. Now just count them up \n",
    "\n",
    "        # count up the numbers of actions this week by domain and week\n",
    "        groupup = dict()\n",
    "        for i in range(len(actweekno)):\n",
    "            if f'{newdomain[i]} {actweekno[i]}' in groupup:\n",
    "                groupup[f'{newdomain[i]} {actweekno[i]}'] += 1\n",
    "            else:\n",
    "                groupup[f'{newdomain[i]} {actweekno[i]}'] = 1\n",
    "\n",
    "        groupupkey = list(groupup.keys())\n",
    "        groupupval = list(groupup.values())\n",
    "\n",
    "        # create lists that contain just domain name and week number\n",
    "        out1 = list()\n",
    "        out2 = list()\n",
    "\n",
    "        for i in range(len(groupupkey)):\n",
    "            out1.append(groupupkey[i].split()[0])\n",
    "            out2.append(groupupkey[i].split()[1])\n",
    "\n",
    "        # export the wrangled file as a csv (each of these files are wrangled version of the raw data files (of each of the client's recorded activity in lucidity) in terms of counts per week per domain)\n",
    "        out = pd.DataFrame({'Domain': out1, 'Week': out2, 'COUNT': groupupval})\n",
    "\n",
    "        if mode in [1,2]:\n",
    "            out.to_csv(f'./History/Week {thisweek}/Partial_Output/_2_{filename.split(\".\")[0]}_clean.csv', index = False)\n",
    "\n",
    "        elif mode == 3:\n",
    "            out.to_csv(f'./History/Week {thisweek}/Partial_Output/_2_Users_Inductee_clean.csv', index = False)\n",
    "\n",
    "        else:\n",
    "            out.to_csv(f'./History/Week {thisweek}/Partial_Output/_2_Users_norm_employee_clean.csv', index = False)\n",
    "\n",
    "    # OS housekeeping and running each of the files through wrangle()\n",
    "    if not os.path.exists(f'./History/Week {thisweek}/Partial_Output'):\n",
    "        os.mkdir(f'./History/Week {thisweek}/Partial_Output')\n",
    "\n",
    "    for file in towranglelist1:\n",
    "        wrangle(file, MiniDatesToWeek_DF, 1)\n",
    "\n",
    "    for file in towranglelist2:\n",
    "        wrangle(file, MiniDatesToWeek_DF, 2)\n",
    "\n",
    "    for file in towranglelist3:\n",
    "        wrangle(file, MiniDatesToWeek_DF, 3)\n",
    "\n",
    "    for file in towranglelist4:\n",
    "        wrangle(file, MiniDatesToWeek_DF, 4)\n",
    "\n",
    "    # PART 3: COMBINE PREVIOUSLY WRANGLED DATAFRAMES INTO ONE (FILLING IN WEEKS WITH NO ACTIVITY)\n",
    "\n",
    "    # import all cleaned data\n",
    "    asset = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_assets_clean.csv')\n",
    "    actions = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_action_clean.csv')\n",
    "    competency = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_competency_record_clean.csv')\n",
    "    form_record = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_form_record_clean.csv')\n",
    "    form_templates = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_form_template_clean.csv')\n",
    "    incidents = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_incident_clean.csv')\n",
    "    users = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_users_clean.csv')\n",
    "    users_induct = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_users_Inductee_clean.csv')\n",
    "    users_norm_emp = pd.read_csv(f'./History/Week {thisweek}/Partial_Output/_2_users_norm_employee_clean.csv')\n",
    "\n",
    "    # Find a set of the domain names - for finding the \"earliest recorded date\" of activity/usage for each\n",
    "    set1 = set(asset['Domain'])\n",
    "    set2 = set(actions['Domain'])\n",
    "    set3 = set(competency['Domain'])\n",
    "    set4 = set(form_record['Domain'])\n",
    "    set5 = set(form_templates['Domain'])\n",
    "    set6 = set(incidents['Domain'])\n",
    "    set7 = set(users['Domain'])\n",
    "\n",
    "    fullset = set1.union(set2).union(set3).union(set4).union(set5).union(set6).union(set7)\n",
    "    fullsetlist = list(fullset) # Now have a full set of the domains\n",
    "    fullsetlist.sort()\n",
    "\n",
    "    iteration = [asset, actions, competency, form_record, form_templates, incidents, users]\n",
    "    newiteration = [asset, actions, competency, form_record, form_templates, incidents, users, users_induct, users_norm_emp]\n",
    "\n",
    "    # Find first week recorded and put them in a dictionary of (key: value) = (domain name: first week of activity)\n",
    "    startweek = dict()\n",
    "\n",
    "    # read in data file _3_ from last week (for concatenating this week's data onto)\n",
    "    olddata = pd.read_csv(f'./History/Week {datastartweek-1}/Partial_Output/_3_combined_cleaned_data.csv')\n",
    "    olddoms = list(set(olddata['Domain']))\n",
    "\n",
    "    # Create a template for recording the data (now fill out gaps between start week and week 216 where there is 0 data)\n",
    "    combineddatatemplate = dict()\n",
    "\n",
    "    # first initiate a blank dictionary with all weeks from the week of argument 1 of this script (the date from which we should start counting in the data)\n",
    "    # this is slightly different from InitialTraining because we are adding new weeks on top of last time's data, so even if no activity at all these weeks we still need to include it.\n",
    "    for dom in olddoms:\n",
    "        for j in range(datastartweek, thisweek+1):\n",
    "            combineddatatemplate[f'{dom} {j}'] = 0\n",
    "\n",
    "    # create blank copies of this initialised template dictionary, and fill them in based on counts from the output of PART 2\n",
    "    assetcomb = combineddatatemplate.copy()\n",
    "    actionscomb = combineddatatemplate.copy()\n",
    "    competencycomb = combineddatatemplate.copy()\n",
    "    form_recordcomb = combineddatatemplate.copy()\n",
    "    form_templatescomb = combineddatatemplate.copy()\n",
    "    incidentscomb = combineddatatemplate.copy()\n",
    "    userscomb = combineddatatemplate.copy()\n",
    "    users_inductcomb = combineddatatemplate.copy()\n",
    "    users_norm_empcomb = combineddatatemplate.copy()\n",
    "\n",
    "    dictlist = [assetcomb, actionscomb, competencycomb, form_recordcomb, form_templatescomb, incidentscomb, userscomb, users_inductcomb, users_norm_empcomb]\n",
    "\n",
    "    # Now fill in the details where there are records (because all dictionary slots initialised, only need to repalce data for weeks where there was a count recorded, and all other are fine to be left untouched - just ends up being 0)\n",
    "    for k in range(len(dictlist)):\n",
    "        dom = list(newiteration[k]['Domain'])\n",
    "        week = list(newiteration[k]['Week'])\n",
    "        count = list(newiteration[k]['COUNT'])\n",
    "\n",
    "        for i in range(len(dom)):\n",
    "            dictlist[k][f'{dom[i]} {week[i]}'] = count[i]\n",
    "\n",
    "    uniqueid = list(assetcomb.keys())\n",
    "    assetcount = list(assetcomb.values())\n",
    "    actioncount = list(actionscomb.values())\n",
    "    competencycount = list(competencycomb.values())\n",
    "    form_recordcount = list(form_recordcomb.values())\n",
    "    form_templatescount = list(form_templatescomb.values())\n",
    "    incidentscount = list(incidentscomb.values())\n",
    "    userscount = list(userscomb.values())\n",
    "    users_inductcount = list(users_inductcomb.values())\n",
    "    users_norm_empcount = list(users_norm_empcomb.values())\n",
    "\n",
    "    # create two more lists that contain just domain and just week - maximises chances of making future wrangling easier\n",
    "    doms = []\n",
    "    weekss = []\n",
    "\n",
    "    for i in range(len(uniqueid)):\n",
    "        doms.append(uniqueid[i].split()[0])\n",
    "        weekss.append(uniqueid[i].split()[1])\n",
    "\n",
    "    # Create a new column for counting the number of weeks since particular company started at Lucidity\n",
    "    selfweeks = []\n",
    "    count = 0\n",
    "\n",
    "    # make all the selfweek number for this new data an impossible value of -1\n",
    "    prev = doms[0]\n",
    "    for i in range(len(doms)):\n",
    "        selfweeks.append(-1)\n",
    "\n",
    "    # turn it into one dataframe and concatanate onto old data\n",
    "    out = pd.DataFrame({'ID': uniqueid, 'Domain': doms, 'Week': weekss, 'Selfweeks': selfweeks,\n",
    "                        'Assets': assetcount, 'Actions': actioncount, 'Competency': competencycount, \n",
    "                        'Form_record': form_recordcount, 'Form_template': form_templatescount,\n",
    "                       'Incident': incidentscount, 'Users': userscount, \n",
    "                        'Users_induction': users_inductcount, 'Users_norm_emp': users_norm_empcount})\n",
    "\n",
    "    combineddata = pd.concat([olddata, out])\n",
    "    combineddata = combineddata.sort_values(['Domain', 'Week'], axis=0) # Sorting the dataframe by domain and weeks\n",
    "\n",
    "    selfweeks = list(combineddata['Selfweeks'])\n",
    "    domain = list(combineddata['Domain'])\n",
    "\n",
    "    # for any self weeks that are -1 (newly added on), just add one onto it from previous week. (it works since the data is sorted)\n",
    "    prev = domain[0]\n",
    "    for i in range(len(selfweeks)):\n",
    "\n",
    "        if domain[i] == prev and selfweeks[i] == -1:\n",
    "            selfweeks[i] = selfweeks[i-1] + 1\n",
    "\n",
    "        elif domain[i] != prev and selfweeks[i] == -1:\n",
    "            selfweeks[i] = 1\n",
    "\n",
    "        prev = domain[i]\n",
    "\n",
    "    # replacing the column 'Selfweeks' with correct data        \n",
    "    combineddata['Selfweeks'] = selfweeks\n",
    "\n",
    "    # This file now has counts of all activities grouped by week by client/domain, sorted by domian and clients. \n",
    "    combineddata.to_csv(f'./History/Week {thisweek}/Partial_Output/_3_combined_cleaned_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba72030-3b3f-4f83-bf39-fdbb49fa84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipdata(data, NWEEKS, attrb, meth, maniptype, discard, thisweek):\n",
    "    \n",
    "    metadata = ['ID', 'Domain', 'Week', 'Selfweeks']\n",
    "    masterlist = [list() for i in range(NWEEKS+5)]\n",
    "    \n",
    "    skip = 0\n",
    "    if discard:\n",
    "        skip = 26\n",
    "    \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i]['Selfweeks'] > NWEEKS+skip:\n",
    "            \n",
    "            for j in range(len(metadata)):\n",
    "                masterlist[j].append(data.loc[i][metadata[j]])\n",
    "                \n",
    "            for j in range(4, NWEEKS+4):\n",
    "                masterlist[j].append(data.loc[i-(j-4)][attrb])\n",
    "            \n",
    "            masterlist[NWEEKS+4].append(data.loc[i][f'{attrb}T'])\n",
    "    \n",
    "    out = pd.DataFrame()\n",
    "        \n",
    "    for i in range(NWEEKS+5):\n",
    "        if i < 4:\n",
    "            out.insert(i, metadata[i], masterlist[i])\n",
    "        elif i == NWEEKS + 4:\n",
    "            out.insert(i, 'Target', masterlist[i])\n",
    "        else:\n",
    "            out.insert(i, f'{i-4}', masterlist[i])\n",
    "    \n",
    "    if not os.path.exists(f'./History/Week {thisweek}/PreparedData'):\n",
    "        os.mkdir(f'./History/Week {thisweek}/PreparedData')\n",
    "    \n",
    "    out.to_csv(f'./History/Week {thisweek}/PreparedData/{maniptype}_{meth}_{attrb}_{NWEEKS}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef44050-f645-4eb7-b1e8-5013b10319bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = False\n",
    "meth = '-26'\n",
    "\n",
    "if '-26' in meth:\n",
    "    combineddata = combineddata[combineddata['Selfweeks'] > 26]\n",
    "    combineddata.index = range(len(combineddata))\n",
    "\n",
    "    discard = True\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for domain, tmp in combineddata.groupby('Domain'):\n",
    "    tmp.index = range(len(tmp))\n",
    "        \n",
    "    for i in range(len(tmp)-1, -1, -1):\n",
    "        if ((tmp.loc[i]['Assets']) | (tmp.loc[i]['Actions']) | (tmp.loc[i]['Competency']) |\n",
    "                (tmp.loc[i]['Form_record']) | (tmp.loc[i]['Form_template']) |\n",
    "                (tmp.loc[i]['Incident']) | (tmp.loc[i]['Users'])):\n",
    "            break\n",
    "        \n",
    "    data = pd.concat([data, tmp[0:i+1]])\n",
    "\n",
    "stdData = pd.DataFrame()\n",
    "    \n",
    "for domain, compData in data.groupby('Domain'):\n",
    "    \n",
    "    with open(f'./Standardisers/{domain}.pickle', 'rb') as f:\n",
    "              scaler = pickle.load(f)\n",
    "        \n",
    "    compData[['Assets', 'Actions', 'Competency',\n",
    "        'Form_record', 'Form_template', 'Incident', 'Users']] = scaler.transform(compData[['Assets', 'Actions', 'Competency',\n",
    "        'Form_record', 'Form_template', 'Incident', 'Users']])\n",
    "\n",
    "    stdData = pd.concat([stdData, compData])\n",
    "\n",
    "\n",
    "with open(f'./Models/attbs.pickle', 'rb') as f:\n",
    "    attrbs = pickle.load(f)\n",
    "\n",
    "    \n",
    "for colName in stdData.columns[4:]:\n",
    "\n",
    "        target = list()\n",
    "\n",
    "        for domain, compData in stdData.groupby('Domain'):\n",
    "            \n",
    "            compData.index = compData['Selfweeks']\n",
    "            \n",
    "            index = compData.index\n",
    "            \n",
    "            out = [(compData.loc[i+1][colName]-compData.loc[i][colName]) if (i+1 in index)\n",
    "                   else np.nan for i in index]\n",
    "            target.extend(out)\n",
    "        \n",
    "        stdData[f'{colName}T'] = target\n",
    "\n",
    "stdData['Week'].astype(int)\n",
    "stdData = stdData.sort_values(['Domain', 'Week'])\n",
    "stdData.index = range(len(stdData))\n",
    "        \n",
    "for attrb in attrbs:\n",
    "    for nWeeks in [11]:\n",
    "        manipdata(stdData, nWeeks, attrb, meth, 'S_D', True, thisweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f06707-e871-4420-b21e-73c2d3469a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Assets', 'Actions', 'Competency',\n",
    "       'Form_record', 'Form_template', 'Incident', 'Users']\n",
    "\n",
    "r = 'r2'\n",
    "nweeks = 11\n",
    "drop = '-26'\n",
    "\n",
    "\n",
    "# Training\n",
    "models = dict()\n",
    "\n",
    "# Test data\n",
    "test = pd.read_csv(f'./History/Week {thisweek}/PreparedData/S_D_{drop}_Assets_{nweeks}.csv')\n",
    "test = test[test['Week'] == thisweek]\n",
    "out = test[test.columns[0:4]]\n",
    "\n",
    "for col in attrbs:\n",
    "    test = pd.read_csv(f'./History/Week {thisweek}/PreparedData/S_D_{drop}_{col}_{nweeks}.csv')\n",
    "    test = test[test['Week'] == thisweek]\n",
    "\n",
    "    xTest = test[test.columns[4:-1]]\n",
    "\n",
    "    with open(f'./Models/{col}.pickle', 'rb') as f:\n",
    "        obj = pickle.load(f)    \n",
    "\n",
    "    lm = obj[0]\n",
    "\n",
    "    yPred = lm.predict(xTest)\n",
    "\n",
    "    out[f'{col}P'] = yPred\n",
    "\n",
    "out['PredScore'] = [0 for i in range(len(out))]\n",
    "\n",
    "if r == 'r':\n",
    "    for col in attrbs:\n",
    "        with open(f'./Models/{col}.pickle', 'rb') as f:\n",
    "            obj = pickle.load(f)    \n",
    "\n",
    "        R = obj[1]\n",
    "\n",
    "        out['PredScore'] = out['PredScore'] + R*out[f'{col}P']\n",
    "\n",
    "elif r == 'r2':\n",
    "    for col in attrbs:\n",
    "        with open(f'./Models/{col}.pickle', 'rb') as f:\n",
    "            obj = pickle.load(f)    \n",
    "\n",
    "        R = obj[1]\n",
    "\n",
    "        out['PredScore'] = out['PredScore'] + (R**2)*out[f'{col}P']\n",
    "\n",
    "else:\n",
    "    for col in attrbs:\n",
    "        out['PredScore'] = out['PredScore'] + out[f'{col}P']\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "for week, data in out.groupby(['Week']):\n",
    "    data.index = range(len(data))\n",
    "\n",
    "    P95 = np.quantile(data['PredScore'], .95)\n",
    "    P05 = np.quantile(data['PredScore'], .05)\n",
    "\n",
    "    data['Predictions'] = ['Increase' if data.loc[i]['PredScore'] > P95 else 'Decrease' if data.loc[i]['PredScore'] < P05 else 'Normal' for i in range(len(data))]\n",
    "\n",
    "    predictions = pd.concat([predictions, data])\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9368d1-60aa-4668-954e-a425ba8182b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/8w_3dndd6l5c3n99vd7vd3f40000gn/T/ipykernel_26266/3076020057.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out['Predictions'] = predictions['Predictions']\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions.sort_values(['Domain'], axis=0)\n",
    "\n",
    "# adding on the Client and Client code for easier mapping later on\n",
    "match = pd.read_excel('./Mapping.xlsx')\n",
    "\n",
    "finaldom = list(predictions['Domain'])\n",
    "\n",
    "dommatchdict1 = dict() # dictionary for storing client name\n",
    "dommatchdict2 = dict() # dictionary for storing client id\n",
    "\n",
    "for i in range(len(finaldom)):\n",
    "    dommatchdict1[finaldom[i]] = ''\n",
    "    dommatchdict2[finaldom[i]] = ''\n",
    "\n",
    "domain1 = list(match['Domain1'])\n",
    "clientname = list(match['Client Code'])\n",
    "clientid = list(match['Client'])\n",
    "for i in range(len(domain1)):\n",
    "    if domain1[i] in dommatchdict1:\n",
    "        dommatchdict1[domain1[i]] = clientname[i]\n",
    "        dommatchdict2[domain1[i]] = clientid[i]\n",
    "\n",
    "dommatchlist1 = list(dommatchdict1.values())\n",
    "dommatchlist2 = list(dommatchdict2.values())\n",
    "\n",
    "predictions.insert(1, \"Client id\", dommatchlist1, True)\n",
    "predictions.insert(2, \"Client name\", dommatchlist2, True)\n",
    "\n",
    "predictions.to_csv(f'./History/Week {thisweek}/PredictionDetails.csv', index = False)\n",
    "\n",
    "out = predictions[predictions.columns[0:4]]\n",
    "out['Predictions'] = predictions['Predictions']\n",
    "\n",
    "out.to_csv('./Predictions.csv', index = False)\n",
    "out.to_csv(f'./History/Week {thisweek}/Predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79ce97-74f7-462a-8a32-571febd2bd80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb89f80-2831-4ac6-a185-1aec6a5e3b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
